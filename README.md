Self-Attention from Scratch

A simple, from-scratch implementation of the self-attention mechanism to find the position of the largest number in a sequence. This educational demo shows how queries, keys, and values let a model focus on important elements, achieving good performance even without fine-tuning.

Key Steps:

Generate and normalize random sequences.

Embed numbers into dense vectors.

Compute query, key, value vectors and attention weights.

Use a feed-forward network to predict the position of the maximum.

Train, evaluate, and visualize attention.

Notes:

Not optimized for performance; purely educational.

Can be adapted for other sequence-focused tasks.
